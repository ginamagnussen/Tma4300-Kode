---
title: "TMA4300 - Exercise 1"
subtitle: "Stochastic simulation"
author: "Marcus A. Engebretsen and Gina Magnussen"
output: 
  pdf_document
    # fig_caption: true
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, autodep = TRUE)
```

```{r, echo=FALSE, include=FALSE}
library(MASS)
library(ggplot2)
library(gridExtra)
```


# Problem A
## Probability integral transform, rejection sampling and bivariate techniques
<!-- ------------------------------------------------------- -->
### 1. Sampling from g(x) - Probability integral transform

The probability density function
\[
g(x) = 
  \begin{cases} 
      cx^{\alpha-1}, & 0 < x < 1 \\
      c e^{-x} & 1 \leq x \\
      0 & \text{otherwise},
   \end{cases} 
\]
is given with $c$ as a normalising constant and $\alpha \in (0,1)$. 

#### a) 
The cumulative distribution function $G_{X}(x)$ is then found by integrating over the different domains and taking into account that the cdf must be continuous, 
\[
G_{X}(x) = 
  \begin{cases}
    \frac{c}{\alpha} & 0 < x < 1, \\
    \frac{c}{\alpha} + ce^{-1} - ce^{-x} & 1 \leq x \\
    0 & \text{else}
  \end{cases}
\]
By using the property that the area under a pdf should integrate to 1, we find that
$c = \frac{e\alpha}{e+\alpha}$

The probability integral transform, setting $u = G_{X}(x)$ and solving for $x$, gives
\[
\begin{aligned}
  x &= G^{-1}(u) =
  \left(u\frac{e+\alpha}{e}\right)^{\frac{1}{\alpha}},  &0 < x < 1 \\
  x &= G^{-1}(u) = -\log \left[\left(\frac{e+\alpha}{e
  \alpha}\right)-\frac{u}{c}\right]=-\log\left[\frac{1}{c}(1-u)\right]  &1 \leq x \\
\end{aligned}
\]

<!-- ------------------------------------------------------- -->
#### b) R function

```{r, echo = TRUE}
# #------Probability integral transform--------#
# Initialize
n <- 100000
alpha <- 0.5
x <- sort(runif(n,0.01,8))

# Functions and sampling
gsample <- function(n,alpha){
  c <- (exp(1)*alpha)/(exp(1)+alpha)
  u <- runif(n)
  
  x1 <- (u*(alpha/c))^(1/alpha)
  x2 <- -log((1/alpha+1/(exp(1))-u/c))
  
  x <- matrix(0,n,1)
  
  for (i in 1:n){
    if(u[i]>= (c/alpha)){
      x[i] <- x2[i]
    }
    else {
      x[i] <- x1[i]
    }
  }
  return(x)
}

xsample2 <- gsample(n, alpha) 

# Check sampling
gfunc <- matrix(0,n,1)

for (i in 1:n){
  if(x[i] <= 0){
    gfunc[i] <- 0
  }
  else if (x[i] < 1) {
    gfunc[i] <- ((exp(1)*alpha)/(exp(1)+alpha))*(x[i]^(alpha-1))
  }
  else {
    gfunc[i] <- ((exp(1)*alpha)/(exp(1)+alpha))*exp(-x[i])
  }
}
```

```{r}
# Plot
dataf <- data.frame(x, gfunc, xsample2)
g <- ggplot(dataf)
g <- g + geom_histogram(aes(xsample2, y = ..density..), binwidth = 0.02, col = "black", fill = "grey", alpha = 0.9)
g <- g + geom_line(aes(x, gfunc), col = "red")
g <- g + coord_cartesian(xlim = range(0:3)) + labs("hei")
g

b <- truehist(xsample2, xlab = "Samples of g(x), alpha = 0.5",main = "Probability integral transform", xlim = range(0:3), cex.main = 1.5, cex.lab = 1.5)
lines(x,gfunc, col = "red", lwd = 3)

#grid.arrange(g)
```






<!-- ------------------------------------------------------- -->
### 2. Gamma distribution with $\alpha \in (0,1)$, $\beta = 1$ generated by rejection sampling
\[
f(x) =
  \begin{cases}
  \frac{1}{\Gamma(\alpha)}x^{\alpha-1}e^{-x}, & 0 < x \\
  0 & \text{else}
  \end{cases}
\]
with $\alpha \in (0,1)$ and $\beta = 1$.

<!-- ------------------------------------------------------- -->
#### a)
The acceptance probability $R$:
\[
  \frac{1}{\gamma}\frac{f(x)}{g(x)}
\]
To maximize $R$, we get by the constraint $f(x) \leq \gamma \cdot g(x)$:
\[
  \gamma = \frac{f(x)}{g(x)} =
    \begin{cases}
  \frac{1}{\Gamma(\alpha)}\frac{e^{-x}}{c} & 0 < x < 1 \\
  \frac{1}{\Gamma(\alpha)}\frac{x^{\alpha-1}}{c} & x \geq 1 
  \end{cases}
\]


\[
  \frac{\partial{\gamma}}{\partial x} = 
    \begin{cases}
  -\frac{1}{\Gamma(\alpha)}\frac{e^{-x}}{c} & 0 < x < 1 \\
  \frac{\alpha-1}{\Gamma(\alpha)}\frac{x^{\alpha-2}}{c} & x \geq 1 
  \end{cases}
\]

Solving $\frac{\partial{\gamma}}{\partial x} = 0$ for $x$ and inserting into the expression for $R$ will now maximize the acceptance probability. 

\[
\gamma = 
\begin{cases}
  \frac{1}{\Gamma(\alpha)}\frac{1}{c} & 0<x<1 \\
  \frac{1}{\Gamma(\alpha)}\frac{1}{c} & x \geq 1
\end{cases}
\]
which in turn leads to
\[
R = 
\begin{cases}
e^{-x} & 0<x<1 \\
x^{\alpha-1} & x \geq 1
\end{cases}
\]

#### b) R function
```{r}
# ---------Utilities -----------# 
acceptanceProb <- function(x,alpha){
  if (x < 1) {
    return (exp(-x))
  }
  return (x^(alpha-1))
}

# -------------- Main function ---------------#
sampleGammaRejection <- function(alpha, n) {
  xsample <- matrix(0,n,1)
  for (j in 1:n){
    fin <- 0
    while (fin == 0){
      xsample[j] <- gsample(1,alpha)
      acc <- acceptanceProb(xsample[j], alpha) # Acceptance prob
      u <- runif(1)
      if (u <= acc){
        fin = 1
      }
    }
  }
  return (xsample)
}
```

```{r}
# Sample
xsample <- sampleGammaRejection(alpha, n)


#Check sampling
gammaf <- function(alpha,x){
  res <- (1/gamma(alpha))*(x^(alpha-1))*exp(-x)
  return(res)
}
y <- gammaf(alpha, x)
check <- data.frame(mean = mean(xsample), truemean = alpha, var = var(xsample), truevar = alpha)
print(check)

#Plot
truehist(xsample, main = "Rejection sampling", xlab = "Samples of g(x), alpha = 0.5", 
         xlim = range(0:2.5), cex.main = 1, cex.lab = 1)
lines(x,y, col = "red", lwd = 3)
```




<!-- ------------------------------------------------------- -->
### 3. Ratio of uniforms - gamma, $\alpha > 1, \beta = 1.$

Consider now the same distribution, but with parameters $\alpha > 1$ and $\beta = 1$. The ratio of uniforms method is used to simulate samples from this distribution. $C_f$, $f^{*}$, $a$, $b_{+}$ and $b_{-}$ as given in formula (3) and (4) in the exercise description.

Need to find maximum of $f^{*}(x)$ to find the bounds for the area $C_f$. Since $f^{*}(x)$ is a concave function, we solve
\[
\begin{aligned}
\frac{df^{*}(x)}{dx} &= (\alpha-1)x^{\alpha-2}-x^{\alpha-1}e^{-x} = 0 &\implies x = \alpha-1 \\
\frac{d(x^2 f^{*}(x))}{dx} &= 0 &\implies x = \alpha + 1
\end{aligned}
\]


\[
\begin{aligned}
f^{*}(\alpha-1) &=(\alpha-1)^{\alpha-1}e^{1-\alpha} \\
a &= \sqrt{\sup_{x} f^{*}(x)} = \sqrt{f^{*}(\alpha-1)} = \sqrt{(\alpha-1)^{\alpha-1}e^{1-\alpha}} \\
b_{+} &= \sqrt{\sup_{x\geq 0} x^2 f^{*}(x)} = \sqrt{(\alpha+1)^2f^{*}(\alpha+1)} = (\alpha+1)^{\frac{\alpha+1}{2}}e^{-\frac{\alpha+1}{2}} \\
b_{-} &= -\sqrt{\sup_{x\leq 0} x^2 f^{*}(x)} = 0
\end{aligned}
\]

\[
C_f = [0, a]\times[b_{-},b_{+}] = \left[0,\sqrt{(\alpha-1)^{\alpha-1}e^{1-\alpha}}\right]\times \left[0, (\alpha+1)^{\frac{\alpha+1}{2}}e^{-\frac{\alpha+1}{2}} \right]
\]

Ratio of uniforms:
\[
x_1 = \frac{u_2}{u_1} \hspace{3cm} x_2 = u_1
\]

```{r}
# ---------- Utilities ------------ #
f_star <- function(x, alpha){
  return (((alpha-1)/2)*x -exp(x)/2)
}

# ----------Main function ------------ #
sampleGammaRatioUniforms <- function(alpha, n) { 
  
  # Determine bounds on a log scale
  log_a_pluss <- (alpha-1)/2 * (log(alpha-1)-1)
  #log_a_minus <- -Inf
  log_b_pluss <- (alpha+1)/2 * log((alpha+1)*exp(-1))
  #log_b_minus <- -Inf # log of both limits...
  
  # Iterations
  count <- 1
  it <- 1
  
  xSample <- matrix(0,n,1)
  
  # Until we have n realisations
  while (it <= n) {
    
    # Uniform sampling
    log_u_1 <- log(runif(1)) + log_a_pluss
    log_u_2 <- log(runif(1)) + log_b_pluss
    
    # Change of variables
    # x_2 <- log_u_1
    # x_1 <- log_u_2 - log_u_1
    log_x <- log_u_2 - log_u_1 #log(u2/u1)
    
    # Acceptance
    if (log_u_1 <= f_star(log_x, alpha)) {
      
      xSample[it] <- log_x
      
      # Increment
      it <- it + 1
      #print(log_u_1 - f_star(log_x, alpha)); flush.console()
      count <- count + 1
    }
    else {
      # Increment count
      count <- count + 1
    }
  }
  struct <- data.frame(xSample = exp(xSample) , count)
  
  return (struct)
}
```

```{r, eval = FALSE}
# ---------- Utilities ------------ #
n <- 1000 # No. of samples
alphamax <- 2000
counts <- matrix(0,1,alphamax-1)

# Generating realisations for set of alphas
count_it <- 0
# for (alpha in 2:alphamax) {
#   
#   out <- sampleGammaRatioUniforms(alpha, n)
#   count_it <- count_it + 1
#   counts[count_it] <- out$count[1] 
#   
# }

frame <- data.frame(alpha = 2:alphamax, counts = counts)
plot(2:alphamax, counts, main = "Rejection sampling: Number of iterations to achieve 1000 samples", 
     xlab = "Alpha", ylab = "Counts", col = "red")

```

### 4: Complete function for gamma sampling
The only case left to solve for the gamma distribution, is when $\alpha = 1$. This leads to a special case, the exponential distribtion where $\beta = \lambda$, which we already know how to sample from. 
```{r}
# Exponential distribution
sampleExponential <- function(lambda, n){
  x <- -(1/lambda)*log(runif(n))
  return(x)
}

# ----- Main function: Gamma sampling -----#
sampleGamma <- function(alpha, beta, n) {
  
  if (alpha > 1) {
    xFrame <- sampleGammaRatioUniforms(alpha,n)
    xSample <- xFrame$xSample
    
  }
  else if ( alpha < 1) {
    xSample <- sampleGammaRejection(alpha, n)
  }
  else {
    xSample <- sampleExponential(lambda = 1, n)
  }
  
  return (beta*xSample)
}
```


<!--------------------------->
# Problem B
## The Dirichlet distribution: Simulating using known relations
\[
  \begin{aligned}
  f_{z}(z, \alpha) &= dz_1 ... dz_k \propto
  (z_1^{\alpha_1-1})e^{-z_1}...(z_k^{\alpha_k-1})e^{-z_k}dz_1...dz_k \\
  &= z_1^{\alpha_1-1}...z_k^{\alpha_k-1}e^{-(z_1+...+z_k)}dz_1...dz_k \\
  &=z_1^{\alpha_1-1}...z_k^{\alpha_k-1}e^{-v}dz_1...dz_k \\
  \text{where} \hspace{2mm}v = -(z_1+...+z_k)
  \end{aligned}
\]

Change of variables
\[
  z_i = x_i\cdot v \implies dz_i = dx_i\cdot v + x_i dv
\]

Using $\sum\limits_{i=1}^k x_i = 1$ and $dx_1+...+dx_k = 0$
Define $w = dz_1 + ... + dz_{k-1}= [dx_1+...+dx_{k-1}]v + [x_1 + ... + x_k]dv$
  
Then
\[
  \begin{aligned}
  dz_k &= dx_kv+x_kv \\
  &=-[dx_1+...+dx_{k-1}]v + [1-[x_1+...x_{k-1}]]dv \\
  &= dv - ([dx_1+...+dx_{k-1}]v+[x_1+...x_{k-1}]dv) \\
  &= dv - w
  \end{aligned}
\]

Using exterior algebra:
\[
  \begin{aligned}
  dz_1 \wedge ...\wedge dz_{k-1} \wedge dz_k &=(dz_1 \wedge...\wedge dz_{k-1})\wedge(dv-w) \\
  &= ... \\
  &= v^{k-1} dx_1 \wedge ... \wedge dx_{k-1} \wedge dv
  \end{aligned}
\]

Filling into the expression gives:
\[
\begin{aligned}
f_{z}(z,\alpha) dz_1...dz_k &\propto 
(x_1v)^{\alpha_1-1}...(x_{k-1}v)^{\alpha_{k-1}-1}(v(1-[x_1+...+x_{k-1}]))^{\alpha_k-1}e^{-v^{k-1}}dx_1 \wedge ... \wedge dx_{k-1} \wedge dv \\
&= v^{\alpha_1 + ...+ \alpha_{k-1}}e^{-v}dv \left(x_1^{\alpha_1-1}...x_{k-1}^{\alpha_{k-1}-1}\right)\left(1-\sum\limits_{i=1}^{k-1}x_i\right)^{\alpha_{k-1}}dx_1...dx_{k-1}dv
\end{aligned}
\]


%% exersice1dirichlet.R 



# Problem C
## A toy Bayesian model: Birthdays
The probability that two or more students has their birthday on the same day can be simulated as follows: Randomly assign a birth date to the $35$ in a given NTNU class, and check for duplicate dates. If so, count this event as a success. If not, assign dates randomly again and to the same check. Do this many times. The estimate is then the number of successes divided by the number of trials.

### 1: Independent birthdays, equally likely
#### a)
```{r, eval = FALSE}
# Birthdays
sim <- 10000
stud <- 35
count <- 0

for (i in 1:sim){
  bdays <- round(runif(stud)*365)
  if (sum(duplicated(bdays))>=1){
    count <- count + 1
  }
}

prob <- count/sim
print(prob)
```
#### b)
Exact calculation:
The probability that two or more students has their birthday on the same day, is the complement of the event that no students has their birthday on the same day. Thus
\[
\begin{aligned}
P(\text{no. of students with same birthday} \geq 2) &= 1 - P(\text{no students with same birthday}) \\
&= 1 - \frac{365!}{330!\hspace{1mm}365^{35}} = 0.8144
\end{aligned}
\]

The difference between the simulated and exact answer is of order $10^{-3}$ or better with enough simulations, meaning the simulated probability is good. 


### 2: Bayesian model
#### a)
The posterior distribution of $(q_1, q_2, q_3, q_4)$, the distribution of the probabilities of being born in a specific season given observations, is 
\[
\begin{aligned}
p(q_{1:4}|x_{1:4}) &= \frac{p(x_{1:4}|q_{1:4})P(q_{1:4})}{\int p(x_{1:4}|q_{1:4})P(q_{1:4})dq_{1:4}} \\
&\propto p(x_{1:4}|q_{1:4})P(q_{1:4}) \\
&\propto \prod_{i=1}^{k}q_i^{x_i}\cdot \prod_{i=1}^{k}q_{i}^{\alpha_i-1} \\
&\propto \prod_{i=1}^{k} q_i^{x_i+\alpha_i-1} \\
&\propto \prod_{i=1}^{k} q_i^{\bar{\alpha_i}-1}
\end{aligned}
\]
 where $(x_{1:4}) = (x_1, x_2, x_3, x_4)$ and similary for $q$. Thus, the posterior distribution of $(q_1, q_2, q_3, q_4)$ is a Dirichlet distribution with parameters $\bar\alpha{i} = x_i + \alpha_i$, $i = 1,...,4$. 

#### b) 
When the joint posterior distribution of $q_{1:4}$ is Dirichlet, the marginal distribution for each season is $q_i \sim beta \left(\alpha_i, \sum\limits_{k=1}^{4}\alpha_k - \alpha_i\right)$.

These marginal distributions are plotted by simulating several Dirichlet distributions $x = (x_1,...,x_K)$, extracting for instance all $x_1$s, which are beta distributed with the same parameters, and making a histogram of these samples. The built-in function in R is used to verify that the simulation of the beta distributions, and thus also the Dirichlet distribution. Another option is to simulate the marginal beta distributions by using the relation between the gamma and the beta distribution, since we in the previous tasks have shown this sample generator to be reasonably correct. 

%% sampleBeta.R



### 3: Sample/Estimate p

Given the probabilities $q_i$, of being born within a specific season we want to find the probability of $p$. This is here done by simulation. 
Denote
\[
N_j = \text{no. of students born in season j,} \hspace{2mm} j \in \{1 = \text{spring}, ..., \text{4 = winter} \} 
\]
and set $m = 35$ again. 


#### a) Sampling from posterior distribution $P(q_1,q_2,q_3,q_4|x_1,x_2,x_3,x_4)$ and $P(N_1,N_2,N_3,N_4|q_1,q_2,q_3,q_4)$))

Sampling from posterior distribution $p(q_1,q_2,q_3,q_4|x_1,x_2,x_3,x_4)$ 

KODE
%% sampleMultinomial.R

As in C.2, the distribution of $(N_1,N_2,N_3,N_4|q_1,q_2,q_3,q_4)$ will be multinomial with parameters $(m,q_1,q_2,q_3,q_4)$, but with $m = 35$.

$(q_1,q_2,q_3,q_4) \sim Dirichlet(\alpha_1,\alpha_2,\alpha_3,\alpha_4)$
$(N_1,N_2,N_3,N_4) \sim multinomial(m, q_1,q_2,q_3,q_4)$

How to sample from a multinomial distribution:
Divide the unit interval $[0,1]$ into four subsequent intervals <!--->
$[0,q_1]\cup(q_1, q_1+q_2]\cup(q_1+q_2, q_1+q_2+q_3]\cup(q_1+q_2+q_3, q_1+q_2+q_3+q_4]$
<!--->
with length $q_1,q_2,q_3,q_4$ respectively. Then sample $u\sim Unif[0,1]$ and increment $N_j$ in the corresponding interval, in this case season, for $j = 1,...,4$. That is, if $u$ is between $q_1$ and $q_1+q_2$, then $N_2$ is incremented. Do this $m = 35$ times.

#### b) 
A formula for
\[
p = f(N_1, N_2, N_3, N_4)
\]
can be found by seeing that
\[
\begin{aligned}
p &= P \left[ \left( Y_1 \cup Y_2 \cup Y_3 \cup Y_4 \right) =1\right]\\
&= 1- P\left[ \left( Y_1 \cup Y_2 \cup Y_3 \cup Y_4 \right) =0\right]
\end{aligned}
\]

where $Y_j$ is the event that two or more students have the same birthday in season $j$.

Since birthdays between different students are independent of each other, we can assume that the event of no students having the same birthday in the spring is independent of the event that no students have the same birthday in the winter, since the events are disjoint. 
So 
\[
p = 1- \left[P(Y_1=0) + P(Y_2 = 0) + P(Y_3=0) + P(Y_4=0)\right]
\]
Now, since the outcome of $Y_i$ is binary, we can render the above equation to a function of $N_i$ using that $K_i$ is the number of days in each season. This gives

\[
\begin{aligned}
P(Y_i = 0) &= \frac{K_i(K_i-1)(K_i-2)...(K_i-(N_i-1))}{K_i!} \\
&= {K_i\choose N_i} \cdot \frac{N_i!}{K_i^{N_i}}
\end{aligned}
\]
and so
\[
p = 1-\left[\sum\limits_{i=1}^{4} {K_i \choose N_i } \cdot \frac{N_i!}{K_i^{N_i}}\right]
\]

Now $p$ can be calculated by sampling from $(N_1,N_2,N_3,N_4|q_1,q_2,q_3,q_4)$, the multinomial distribution, calculating $p$ by using the formula above, repeating this $n$ times and then calculate the mean. 

Confidence interval of posterior mean of $p$: Sort all the $n$ found values for $p$, and then remove the $2.5\%$ highest and lowest values. 

Posterior mean is larger than the probability in C.1. This is reasonable because the probabilities $q$ determine how likely it is to have a birthday within one season. If one $q_i$ is larger than another, there are more chances of birthdays colliding within that season, and thus the probability of two or more people having the same birthday increases. 

%% sampleP.R
%% exercise1sampelPosteriorMean.R


#### c)
Assume now a Dirichlet prior with $\alpha_1 = \alpha_2 = \alpha_3 = \alpha_4 = 20$.

Importance sampling

Comment and explain result:
- Small alphas, all equal, will not affect the distribution that much, the observations have a lot to say
- Bigger alphas, if all are equal, will give uniformly distributed birthdays between seasons, and push the $q_i$s closer to $0.25$ (since the observations become less important.) Meaning we should get a similar result as in C1. 
$\alpha_i$ significantly bigger than the others alphas will increase the $q_i$ (seen in pdf expression for $q|x$), the chance of colliding birthdays is bigger and thus p increases. 

%% importanceSampling.R
tle: "TMA4300 - Exercise 1"
subtitle: "Stochastic simulation"
author: "Marcus A. Engebretsen and Gina Magnussen"
output: 
  pdf_document
    # fig_caption: true
---
```{r, echo=FALSE, include=FALSE}
library(MASS)
library(ggplot2)
```


# Problem A
## Probability integral transform, rejection sampling and bivariate techniques
<!-- ------------------------------------------------------- -->
### 1. Sampling from g(x) - Probability integral transform

The probability density function
\[
g(x) = 
  \begin{cases} 
      cx^{\alpha-1}, & 0 < x < 1 \\
      c e^{-x} & 1 \leq x \\
      0 & \text{otherwise},
   \end{cases} 
\]
is given with $c$ as a normalising constant and $\alpha \in (0,1)$. 

#### a) 
The cumulative distribution function $G_{X}(x)$ is then found by integrating over the different domains and taking into account that the cdf must be continuous, 
\[
G_{X}(x) = 
  \begin{cases}
    \frac{c}{\alpha} & 0 < x < 1, \\
    \frac{c}{\alpha} + ce^{-1} - ce^{-x} & 1 \leq x \\
    0 & \text{else}
  \end{cases}
\]
By using the property that the area under a pdf should integrate to 1, we find that
$c = \frac{e\alpha}{e+\alpha}$

The probability integral transform, setting $u = G_{X}(x)$ and solving for $x$, gives
\[
\begin{aligned}
  x &= G^{-1}(u) =
  \left(u\frac{e+\alpha}{e}\right)^{\frac{1}{\alpha}},  &0 < x < 1 \\
  x &= G^{-1}(u) = -\log \left[\left(\frac{e+\alpha}{e
  \alpha}\right)-\frac{u}{c}\right]=-\log\left[\frac{1}{c}(1-u)\right]  &1 \leq x \\
\end{aligned}
\]

```{r, eval = FALSE, include=FALSE}
# #------Probability integral transform--------#
# Initialize
n <- 100000
alpha <- 0.5
x <- sort(runif(n,0.01,8))

# Functions and sampling
gsample <- function(n,alpha){
  c <- (exp(1)*alpha)/(exp(1)+alpha)
  u <- runif(n)
  
  x1 <- (u*(alpha/c))^(1/alpha)
  x2 <- -log((1/alpha+1/(exp(1))-u/c))
  
  x <- matrix(0,n,1)
  
  for (i in 1:n){
    if(u[i]>= (c/alpha)){
      x[i] <- x2[i]
    }
    else {
      x[i] <- x1[i]
    }
  }
  return(x)
}

xsample2 <- gsample(n, alpha) 

# Check sampling
gfunc <- matrix(0,n,1)

for (i in 1:n){
  if(x[i] <= 0){
    gfunc[i] <- 0
  }
  else if (x[i] < 1) {
    gfunc[i] <- ((exp(1)*alpha)/(exp(1)+alpha))*(x[i]^(alpha-1))
  }
  else {
    gfunc[i] <- ((exp(1)*alpha)/(exp(1)+alpha))*exp(-x[i])
  }
}



# Plot
dataf <- data.frame(x, gfunc, xsample2)
# g <- ggplot(dataf)
# g <- g + geom_histogram(aes(xsample2, y = ..density..), binwidth = 0.01, col = "black", fill = "grey")
# g <- g + geom_line(aes(x, gfunc), col = "red")
# g <- g + coord_cartesian(xlim = range(0:3)) + labs("hei")
# g

truehist(xsample2, xlab = "Samples of g(x), alpha = 0.5",main = "Probability integral transform", xlim = range(0:3), cex.main = 2, cex.lab = 1.5)
lines(x,gfunc, col = "red", lwd = 3)

```



<!-- ------------------------------------------------------- -->
#### b)
R function

R plot: gsample




<!-- ------------------------------------------------------- -->
### 2. Gamma distribution with $\alpha \in (0,1)$, $\beta = 1$ generated by rejection sampling
\[
f(x) =
  \begin{cases}
  \frac{1}{\Gamma(\alpha)}x^{\alpha-1}e^{-x}, & 0 < x \\
  0 & \text{else}
  \end{cases}
\]
with $\alpha \in (0,1)$ and $\beta = 1$.

<!-- ------------------------------------------------------- -->
#### a)
The acceptance probability $R$:
\[
  \frac{1}{\gamma}\frac{f(x)}{g(x)}
\]

\[
  \gamma_1 = \frac{f(x)}{g(x)} =
    \begin{cases}
  \frac{1}{\Gamma(\alpha)}\frac{e^{-x}}{c} & 0 < x < 1 \\
  \frac{1}{\Gamma(\alpha)}\frac{x^{\alpha-1}}{c} & x \geq 1 
  \end{cases}
\]


\[
  \frac{\partial{\gamma_1}}{\partial x} = 
    \begin{cases}
  -\frac{1}{\Gamma(\alpha)}\frac{e^{-x}}{c} & 0 < x < 1 \\
  \frac{\alpha-1}{\Gamma(\alpha)}\frac{x^{\alpha-2}}{c} & x \geq 1 
  \end{cases}
\]

Solving $\frac{\partial{\gamma_1}}{\partial x} = 0$ for $x$ and inserting into the expression for $R$ will now maximize the acceptance probability given the constraint that $f(x) < \frac{1}{\gamma}g(x)$. 
CHECK CONSTANTS!!

\[
\gamma_1 = 
\begin{cases}
  \frac{1}{\Gamma(\alpha)}\frac{1}{c} & 0<x<1 \\
  \frac{1}{\Gamma(\alpha)}\frac{1}{c} & x \geq 1
\end{cases}
\]
which in turn gives an acceptance probability
\[
R = 
\begin{cases}
e^{-x} & 0<x<1 \\
x^{\alpha-1} & x \geq 1
\end{cases}
\]

#### b) R function


<!-- ------------------------------------------------------- -->
### 3. Ratio of uniforms - gamma, $\alpha > 1, \beta = 1.$

Consider now the same distribution, but with parameters $\alpha > 1$ and $\beta = 1$. The ratio of uniforms method is used to simulate samples from this distribution.
With $C_f$, $f^{*}$, $a$, $b_{+}$ and $b_{-}$ as given in formula (3) and (4) in the exercise description, we find that

Need to find maximum of $f^{*}(x)$ to find the bounds for the area $C_f$. Since $f^{*}(x)$ is a concave function, we solve
\[
\frac{df^{*}(x)}{dx} = (\alpha-1)x^{\alpha-2}-x^{\alpha-1}e^{-x} = 0 \implies x = \alpha-1 \\
\frac{d(x^2 f^{*}(x))}{dx} = 0 \implies x = \alpha + 1
\]




\[
\begin{aligned}
f^{*}(\alpha-1) &=(\alpha-1)^{\alpha-1}e^{1-\alpha} \\
a &= \sqrt{\sup_{x} f^{*}(x)} = \sqrt{f^{*}(\alpha-1)} = \sqrt{(\alpha-1)^{\alpha-1}e^{1-\alpha}} \\
b_{+} &= \sqrt{\sup_{x\geq 0} x^2 f^{*}(x)} = \sqrt{(\alpha+1)^2f^{*}(\alpha+1)} = (\alpha+1)^{\frac{\alpha+1}{2}}e^{-\frac{\alpha+1}{2}} \\
b_{-} &= (\sqrt{\sup_{x\leq 0} x^2 f^{*}(x)}?) = 0
\end{aligned}
\]

\[
C_f = [0, a]\times[b_{-},b_{+}] = \left[0,\sqrt{(\alpha-1)^{\alpha-1}e^{1-\alpha}}\right]\times \left[0, (\alpha+1)^{\frac{\alpha+1}{2}}e^{-\frac{\alpha+1}{2}} \right]
\]

Ratio of uniforms:
\[
x_1 = \frac{u_2}{u_1} \hspace{3cm} x_2 = u_1
\]


<!--------------------------->
# Problem B
## The Dirichlet distribution: Simulating using known relations
\[
  \begin{aligned}
  f_{z}(z, \alpha) &= dz_1 ... dz_k \propto
  (z_1^{\alpha_1-1})e^{-z_1}...(z_k^{\alpha_k-1})e^{-z_k}dz_1...dz_k \\
  &= z_1^{\alpha_1-1}...z_k^{\alpha_k-1}e^{-(z_1+...+z_k)}dz_1...dz_k \\
  &=z_1^{\alpha_1-1}...z_k^{\alpha_k-1}e^{-v}dz_1...dz_k \\
  \text{where} \hspace{2mm}v = -(z_1+...+z_k)
  \end{aligned}
\]

Change of variables
\[
  z_i = x_i\cdot v \implies dz_i = dx_i\cdot v + x_i dv
\]

Using $\sum\limits_{i=1}^k x_i = 1$ and $dx_1+...+dx_k = 0$
Define $w = dz_1 + ... + dz_{k-1}= [dx_1+...+dx_{k-1}]v + [x_1 + ... + x_k]dv$
  
Then
\[
  \begin{aligned}
  dz_k &= dx_kv+x_kv \\
  &=-[dx_1+...+dx_{k-1}]v + [1-[x_1+...x_{k-1}]]dv \\
  &= dv - ([dx_1+...+dx_{k-1}]v+[x_1+...x_{k-1}]dv) \\
  &= dv - w
  \end{aligned}
\]

Using exterior algebra:
\[
  \begin{aligned}
  dz_1 \wedge ...\wedge dz_{k-1} \wedge dz_k &=(dz_1 \wedge...\wedge dz_{k-1})\wedge(dv-w) \\
  &= ... \\
  &= v^{k-1} dx_1 \wedge ... \wedge dx_{k-1} \wedge dv
  \end{aligned}
\]

Filling into the expression gives:
\[
\begin{aligned}
f_{z}(z,\alpha) dz_1...dz_k &\propto 
(x_1v)^{\alpha_1-1}...(x_{k-1}v)^{\alpha_{k-1}-1}(v(1-[x_1+...+x_{k-1}]))^{\alpha_k-1}e^{-v^{k-1}}dx_1 \wedge ... \wedge dx_{k-1} \wedge dv \\
&= v^{\alpha_1 + ...+ \alpha_{k-1}}e^{-v}dv \left(x_1^{\alpha_1-1}...x_{k-1}^{\alpha_{k-1}-1}\right)\left(1-\sum\limits_{i=1}^{k-1}x_i\right)^{\alpha_{k-1}}dx_1...dx_{k-1}dv
\end{aligned}
\]





# Problem C
## A toy Bayesian model: Birthdays
The probability that two or more students has their birthday on the same day can be simulated as follows: Randomly assign a birth date to the $35$ in a given NTNU class, and check for duplicate dates. If so, count this event as a success. If not, assign dates randomly again and to the same check. Do this many times. The estimate is then the number of successes divided by the number of trials.

### 1: Independent birthdays, equally likely
#### a)
```{r, eval = FALSE}
# Birthdays
sim <- 10000
stud <- 35
count <- 0

for (i in 1:sim){
  bdays <- round(runif(stud)*365)
  if (sum(duplicated(bdays))>=1){
    count <- count + 1
  }
}

prob <- count/sim
print(prob)
```
#### b)
Exact calculation:
The probability that two or more students has their birthday on the same day, is the complement of the event that no students has their birthday on the same day. Thus
\[
\begin{aligned}
P(\text{no. of students with same birthday} \geq 2) &= 1 - P(\text{no students with same birthday}) \\
&= 1 - \frac{365!}{330!\hspace{1mm}365^{35}} = 0.8144
\end{aligned}
\]

The difference between the simulated and exact answer is of order $10^{-3}$ or better with enough simulations, meaning the simulated probability is good. 


### 2: Bayesian model
#### a)
The posterior distribution of $(q_1, q_2, q_3, q_4)$, the distribution of the probabilities of being born in a specific season given observations, is 
\[
\begin{aligned}
P(q_{1:4}|x_{1:4}) &= \frac{P(x_{1:4}|q_{1:4})P(q_{1:4})}{integral of something P(x_{1:4})} \\
&\propto P(x_{1:4}|q_{1:4})P(q_{1:4}) \\
&\propto \prod_{i=1}^{k}q_i^{x_i}\cdot \prod_{i=1}^{k}q_{i}^{\alpha_i-1} \\
&\propto \prod_{i=1}^{k} q_i^{x_i+\alpha_i-1} \\
&\propto \prod_{i=1}^{k} q_i^{\bar{\alpha_i}-1}
\end{aligned}
\]
 where $(x_{1:4}) = (x_1, x_2, x_3, x_4)$ and similary for $q$. Thus, the posterior distribution of $(q_1, q_2, q_3, q_4)$ is a Dirichlet distribution with parameters $\bar\alpha{i} = x_i + \alpha_i$, $i = 1,...,4$. 
 - What is k? 


#### b) 
When the joint posterior distribution of $q_{1:4}$ is Dirichlet, the marginal distribution for each season is $q_i \sim beta \left(\alpha_i, \sum\limits_{k=1}^{4}\alpha_k - \alpha_i\right)$.

These marginal distributions are plotted by simulating several Dirichlet distributions $x = (x_1,...,x_K)$, extracting for instance all $x_1$s, which are beta distributed with the same parameters, and making a histogram of these samples. The built-in function in R is used to verify that the simulation of the beta distributions, and thus also the Dirichlet distribution, is correct. ?? Built-in or sample? Another option is to simulate the marginal beta distributions by using the relation between the gamma and the beta distribution, since we in the previous tasks have shown this sample generator to be reasonably correct. 




```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

### 3: Sample/Estimate p

Given the probabilities $q_i$, of being born within a specific season we want to find the probability of $p$. This is here done by simulation. 
Denote
\[
N_j = \text{no. of students born in season j,} \hspace{2mm} j \in \{1 = \text{spring}, ..., \text{4 = winter} \} 
\]
and set $m = 35$ again. 


#### a) Sampling from posterior distribution $P(q_1,q_2,q_3,q_4|x_1,x_2,x_3,x_4)$ and $P(N_1,N_2,N_3,N_4|q_1,q_2,q_3,q_4)$))

Sampling from posterior distribution $P(q_1,q_2,q_3,q_4|x_1,x_2,x_3,x_4)$ 

KODE


------------
As in C.2, the distribution of $(N_1,N_2,N_3,N_4|q_1,q_2,q_3,q_4)$ will be multinomial with parameters $(m,q_1,q_2,q_3,q_4)$, but with $m = 35$.

$(q_1,q_2,q_3,q_4) \sim Dirichlet(\alpha_1,\alpha_2,\alpha_3,\alpha_4)\\$
$(N_1,N_2,N_3,N_4) \sim multinomial(m, q_1,q_2,q_3,q_4)$

How to sample from a multinomial distribution:
Divide the unit interval $[0,1]$ into four subsequent intervals <!--->
$[0,q_1]\cup(q_1, q_1+q_2]\cup(q_1+q_2, q_1+q_2+q_3]\cup(q_1+q_2+q_3, q_1+q_2+q_3+q_4]$
<!--->
with length $q_1,q_2,q_3,q_4$ respectively. Then sample $u\sim Unif[0,1]$ and increment $N_j$ in the corresponding interval, in this case season, for $j = 1,...,4$. That is, if $u$ is between $q_1$ and $q_1+q_2$, then $N_2$ is incremented. Do this $m = 35$ times.

#### b) 
A formula for
\[
p = f(N_1, N_2, N_3, N_4)
\]
can be found by seeing that
\[
\begin{aligned}
p &= P \left[ \left( Y_1 \cup Y_2 \cup Y_3 \cup Y_4 \right) =1\right]
&= 1- P\left[ \left( Y_1 \cup Y_2 \cup Y_3 \cup Y_4 \right) =0\right]
\end{aligned}
\]

where $Y_j$ is the event that two or more students have the same birthday in season $j$.

Since birthdays between different students are independent of each other, we can assume that the event of no students having the same birthday in the spring is independent of the event that no students have the same birthday in the winter, since the events are disjoint. 
So 
\[
p = 1- \left[P(Y_1=0) + P(Y_2 = 0) + P(Y_3=0) + P(Y_4=0)\right]
\]
Now, since the outcome of $Y_i$ is binary, we can render the above equation to a function of $N_i$ using that $K_i$ is the number of days in each season. This gives

\[
\begin{aligned}
P(Y_i = 0) &= \frac{K_i(K_i-1)(K_i-2)...(K_i-(N_i-1))}{K_i!} \\
{K_i\choose N_i} \cdot \frac{N_i!}{K_i^{N_i}}
\end{aligned}
\]
and so
\[
p = 1-\left[\sum\limits_{i=1}^{4} {K_i \choose N_i } \cdot \frac{N_i!}{K_i!}\right]
\]

Now $p$ can be calculated by sampling from $(N_1,N_2,N_3,N_4|q_1,q_2,q_3,q_4)$, the multinomial distribution, calculating $p$ by using the formula above, repeating this $n$ times and then calculate the mean. 

Confidence interval of posterior mean of $p$: Sort all the $n$ found values for $p$, and then remove the $2.5\%$ highest and lowest values. 

Posterior mean is larger than the probability in C.1. This is reasonable because the probabilities $q$ determine how likely it is to have a birthday within one season. If one $q_i$ is larger than another, there are more chances of birthdays colliding within that season, and thus the probability of two or more people having the same birthday increases. 


#### c)
Assume now a Dirichlet prior with $\alpha_1 = \alpha_2 = \alpha_3 = \alpha_4 = 20$.

Importance sampling

Comment and explain result:
- Small alphas, all equal, will not affect the distribution that much, the observations have a lot to say
- Bigger alphas, if all are equal, will give uniformly distributed birthdays between seasons, and push the $q_i$s closer to $0.25$ (since the observations become less important.) Meaning we should get a similar result as in C1. 
$\alpha_i$ significantly bigger than the others alphas will increase the $q_i$ (seen in pdf expression for $q|x$), the chance of colliding birthdays is bigger and thus p increases. 



